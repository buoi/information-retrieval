{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMB0wO9ixvB+J15thFJoQut",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "51f6d24a7927464e8afa24724e50645b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e7f0f35b808440d590e059fcad742c83",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_af040ee6f6914306b07c892bd1ac615b",
              "IPY_MODEL_5eb7c4f4262545d1bde27d7146863078"
            ]
          }
        },
        "e7f0f35b808440d590e059fcad742c83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "af040ee6f6914306b07c892bd1ac615b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_96c743e45cce489faf9fb2d9b2024e1a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 127,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 127,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e23e481a6fb942eb89a0ede1b42c5361"
          }
        },
        "5eb7c4f4262545d1bde27d7146863078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_df8be4be330c490e883bf41202cff3a8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 127/127 [00:27&lt;00:00,  4.64it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_681abcaedacd44e098c8d56c4d9daa2a"
          }
        },
        "96c743e45cce489faf9fb2d9b2024e1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e23e481a6fb942eb89a0ede1b42c5361": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "df8be4be330c490e883bf41202cff3a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "681abcaedacd44e098c8d56c4d9daa2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/the-SQuAD-squad/IR/blob/deploy/end2end.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLHm53Q8jleW",
        "outputId": "0f35c477-2940-4598-b9cd-cb1a68962fe0"
      },
      "source": [
        "#@title Init { form-width: \"25%\" }\r\n",
        "\r\n",
        "import os\r\n",
        "import random\r\n",
        "import math\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import json\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "import string\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "\r\n",
        "!pip3 install wikipedia-api > /dev/null\r\n",
        "import wikipediaapi\r\n",
        "\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "pd.set_option('display.max_colwidth', -1)\r\n",
        "\r\n",
        "# fix random seeds\r\n",
        "seed_value = 42 #@param {type:\"integer\"}\r\n",
        "\r\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\r\n",
        "random.seed(seed_value)\r\n",
        "np.random.seed(seed_value)\r\n",
        "\r\n",
        "tf.compat.v1.set_random_seed(seed_value)\r\n",
        "\r\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\r\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\r\n",
        "tf.compat.v1.keras.backend.set_session(sess)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd6tfe7UjPps"
      },
      "source": [
        "#@title Get links from Wiki page { form-width: \"25%\" }\r\n",
        "\r\n",
        "wiki_wiki = wikipediaapi.Wikipedia('en')\r\n",
        "page_py = wiki_wiki.page(\"Tom Cruise filmography\")\r\n",
        "def get_links(page):\r\n",
        "    links = page.links\r\n",
        "    for title in sorted(links.keys()):\r\n",
        "        if links[title].ns != 0 or \"Unauthorized\" in title or \"Being Tom Cruise\" in title:  # remove some links not strictly related to the tom cruise filmography\r\n",
        "            links.pop(title)\r\n",
        "    return links\r\n",
        "\r\n",
        "links = get_links(page_py)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "51f6d24a7927464e8afa24724e50645b",
            "e7f0f35b808440d590e059fcad742c83",
            "af040ee6f6914306b07c892bd1ac615b",
            "5eb7c4f4262545d1bde27d7146863078",
            "96c743e45cce489faf9fb2d9b2024e1a",
            "e23e481a6fb942eb89a0ede1b42c5361",
            "df8be4be330c490e883bf41202cff3a8",
            "681abcaedacd44e098c8d56c4d9daa2a"
          ]
        },
        "id": "utJlwUwUjUmJ",
        "outputId": "d945f67a-e706-4df3-c216-30c9414d3622"
      },
      "source": [
        "#@title Get Wiki pages { form-width: \"25%\" }\r\n",
        "\r\n",
        "pages_text = {}\r\n",
        "for title in tqdm(links):\r\n",
        "    page_py = links[title]\r\n",
        "    pages_text[title] = page_py.text\r\n",
        "\r\n",
        "# remove the last sections of the wiki page from the text\r\n",
        "for title in pages_text:\r\n",
        "    stop_index = pages_text[title].rfind(\"References\")\r\n",
        "    pages_text[title] = pages_text[title][:stop_index]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51f6d24a7927464e8afa24724e50645b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=127.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2pJ12jyjVt7"
      },
      "source": [
        "#@title preprocess wiki pages { form-width: \"25%\" }\r\n",
        "def preprocess_text(text):\r\n",
        "    REPLACE_WITH_SPACE = re.compile(r\"\\n\") \r\n",
        "    text = [REPLACE_WITH_SPACE.sub(\" \", line) for line in text]\r\n",
        "    text = [re.sub(r\"([(.;:!\\'ˈ~?,\\\"(\\[\\])\\\\\\/\\-–\\t```<>_#$€@%*+—°′″“”×’^₤₹‘])\", r'', line) for line in text]\r\n",
        "    # we noticed that in the text sometimes we find numbers and the following word merged together (ex: 1980february),\r\n",
        "    # so we put a space between the number and the word\r\n",
        "    text = [re.sub(r\"(\\d+)([a-z]+)\", r'\\1 \\2', line) for line in text] \r\n",
        "    text = [re.sub('\\s{2,}', ' ', line.strip()) for line in text]   # replacing more than one consecutive blank spaces with only one of them\r\n",
        "    return text\r\n",
        "    \r\n",
        "pages_text_preprocessed = preprocess_text(pages_text.values())\r\n",
        "titles = list(pages_text.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFTlf1TAjXhe",
        "outputId": "ee381929-5788-481d-c2d6-2c81d016e63b"
      },
      "source": [
        "#@title tf-idf wiki { form-width: \"25%\" }\r\n",
        "vectorizer =  TfidfVectorizer()\r\n",
        "\r\n",
        "questions = [\"When was Tom Cruise born?\", \"What was the first film Tom Cruise acted in?\", \"What does Tom Cruise believe in?\", \"What is Tom Cruise character's name in Mission Impossible?\", \"What is Vanilla Sky?\", \"Who directed Mission Impossible?\"]\r\n",
        "\r\n",
        "# fit the vectorizer on the preprocessed wikipedia text\r\n",
        "passages_vectorized = vectorizer.fit_transform(pages_text_preprocessed)\r\n",
        "questions_vectorized = vectorizer.transform(questions)\r\n",
        "\r\n",
        "results = cosine_similarity(questions_vectorized,passages_vectorized)\r\n",
        "\r\n",
        "for i,row in enumerate(results):\r\n",
        "    index = np.argmax(row)\r\n",
        "    print(questions[i], end = \" ---> \")\r\n",
        "    print(titles[index])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When was Tom Cruise born? ---> Tom Cruise\n",
            "What was the first film Tom Cruise acted in? ---> Tom Cruise\n",
            "What does Tom Cruise believe in? ---> Tom Cruise\n",
            "What is Tom Cruise character's name in Mission Impossible? ---> Mission: Impossible (film series)\n",
            "What is Vanilla Sky? ---> Vanilla Sky\n",
            "Who directed Mission Impossible? ---> Mission: Impossible (film series)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYS3AlSujY5X",
        "outputId": "821dc1c4-4c41-4186-9e4e-79f8ef60800d"
      },
      "source": [
        "#@title model definition { form-width: \"25%\" }\r\n",
        "\r\n",
        "!pip install 'Transformers==4.3'\r\n",
        "import transformers\r\n",
        "\r\n",
        "max_seq_length = 512\r\n",
        "pretrained_model_str = \"roberta-base\"\r\n",
        "bert_hf_layer = transformers.TFRobertaModel.from_pretrained(\r\n",
        "    pretrained_model_str, output_attentions=True)\r\n",
        "\r\n",
        "#@title model definition { form-width: \"25%\" }\r\n",
        "\r\n",
        "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\r\n",
        "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\r\n",
        "input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')\r\n",
        "\r\n",
        "#pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\r\n",
        "\r\n",
        "#HUGGINGFACE 🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗\r\n",
        "sequence_output = bert_hf_layer(input_ids=input_word_ids, attention_mask=input_mask, \r\n",
        "                                token_type_ids=input_type_ids).last_hidden_state\r\n",
        "\r\n",
        "#do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\r\n",
        "\r\n",
        "start_logits = tf.keras.layers.Dense(1, name=\"start_logit\", use_bias=False)(sequence_output)\r\n",
        "start_logits = tf.keras.layers.Flatten(name=\"flatten_start\")(start_logits)\r\n",
        "\r\n",
        "end_logits = tf.keras.layers.Dense(1, name=\"end_logit\", use_bias=False)(sequence_output)\r\n",
        "end_logits = tf.keras.layers.Flatten(name=\"flatten_end\")(end_logits)\r\n",
        "\r\n",
        "start_probs = tf.keras.layers.Activation(tf.keras.activations.softmax, name=\"softmax_start\")(start_logits)\r\n",
        "end_probs = tf.keras.layers.Activation(tf.keras.activations.softmax, name=\"softmax_end\")(end_logits)\r\n",
        "\r\n",
        "model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], \r\n",
        "                    outputs=[start_probs, end_probs],\r\n",
        "                    name=\"BERT_QA\")\r\n",
        "\r\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\r\n",
        "\r\n",
        "optimizer = tf.keras.optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\r\n",
        "\r\n",
        "model.summary(line_length=150)\r\n",
        "\r\n",
        "# load weights from the SQuAD v2 training\r\n",
        "!wget https://api.wandb.ai/files/buio/SQUAD/jkgwaatn/model-best.h5\r\n",
        "model.load_weights(\"model-best.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f79117fcec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f79117fcec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f792d0abc20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7f792d0abc20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convertWARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "\n",
            "Model: \"BERT_QA\"\n",
            "______________________________________________________________________________________________________________________________________________________\n",
            "Layer (type)                                     Output Shape                     Param #           Connected to                                      \n",
            "======================================================================================================================================================\n",
            "input_word_ids (InputLayer)                      [(None, 512)]                    0                                                                   \n",
            "______________________________________________________________________________________________________________________________________________________\n",
            "input_mask (InputLayer)                          [(None, 512)]                    0                                                                   \n",
            "______________________________________________________________________________________________________________________________________________________\n",
            "input_type_ids (InputLayer)                      [(None, 512)]                    0                                                                   \n",
            "______________________________________________________________________________________________________________________________________________________\n",
            "tf_roberta_model (TFRobertaModel)                TFBaseModelOutputWithPooling(las 124645632         input_word_ids[0][0]                              \n",
            "                                                                                                    input_mask[0][0]                                  \n",
            "                                                                                                    input_type_ids[0][0]                              \n",
            "______________________________________________________________________________________________________________________________________________________\n",
            "start_logit (Dense)                              (None, 512, 1)                   768               tf_roberta_model[0][12]                           \n",
            "______________________________________________________________________________________________________________________________________________________\n",
            "end_logit (Dense)                                (None, 512, 1)                   768               tf_roberta_model[0][12]                           \n",
            "______________________________________________________________________________________________________________________________________________________\n",
            "flatten_start (Flatten)                          (None, 512)                      0                 start_logit[0][0]                                 \n",
            "______________________________________________________________________________________________________________________________________________________\n",
            "flatten_end (Flatten)                            (None, 512)                      0                 end_logit[0][0]                                   \n",
            "______________________________________________________________________________________________________________________________________________________\n",
            "softmax_start (Activation)                       (None, 512)                      0                 flatten_start[0][0]                               \n",
            "______________________________________________________________________________________________________________________________________________________\n",
            "softmax_end (Activation)                         (None, 512)                      0                 flatten_end[0][0]                                 \n",
            "======================================================================================================================================================\n",
            "Total params: 124,647,168\n",
            "Trainable params: 124,647,168\n",
            "Non-trainable params: 0\n",
            "______________________________________________________________________________________________________________________________________________________\n",
            "--2021-03-13 17:23:20--  https://api.wandb.ai/files/buio/SQUAD/jkgwaatn/model-best.h5\n",
            "Resolving api.wandb.ai (api.wandb.ai)... 35.186.228.49\n",
            "Connecting to api.wandb.ai (api.wandb.ai)|35.186.228.49|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: https://storage.googleapis.com/wandb-production.appspot.com/buio/SQUAD/jkgwaatn/model-best.h5?Expires=1615656260&GoogleAccessId=wandb-production%40appspot.gserviceaccount.com&Signature=LcQTq4uNNfouP22G%2FvbevWpbbupFe5umj3zLkh1cZVt2VIVHeUIR4opJmt35iwJKVM3b2227MNBzv%2Beh2HjYNER8W%2BDEZrRM2TOGMXfElM0p4gb3G4Xi2YBYsRnB2aflBh%2FZFD2Fm64mG%2B1EmYGSfFZJge2J2TbHzAJE66cOQKnPyFZZnxmoOVfk%2Fh2o%2Fc2oMyvYAJirlkwqraUxwxVuD1YyxcxWWnjo12SFztSkhJsCJSNip8QDjw7kSY8j8xfKEeDhNPiqU4F2l9pt046H%2BGSRcojunylgg%2BAIVwGY7lmNNISRZfRVFUvuT3w9OMUoqNce2uMPUx5AAVc3JUvg3A%3D%3D [following]\n",
            "--2021-03-13 17:23:20--  https://storage.googleapis.com/wandb-production.appspot.com/buio/SQUAD/jkgwaatn/model-best.h5?Expires=1615656260&GoogleAccessId=wandb-production%40appspot.gserviceaccount.com&Signature=LcQTq4uNNfouP22G%2FvbevWpbbupFe5umj3zLkh1cZVt2VIVHeUIR4opJmt35iwJKVM3b2227MNBzv%2Beh2HjYNER8W%2BDEZrRM2TOGMXfElM0p4gb3G4Xi2YBYsRnB2aflBh%2FZFD2Fm64mG%2B1EmYGSfFZJge2J2TbHzAJE66cOQKnPyFZZnxmoOVfk%2Fh2o%2Fc2oMyvYAJirlkwqraUxwxVuD1YyxcxWWnjo12SFztSkhJsCJSNip8QDjw7kSY8j8xfKEeDhNPiqU4F2l9pt046H%2BGSRcojunylgg%2BAIVwGY7lmNNISRZfRVFUvuT3w9OMUoqNce2uMPUx5AAVc3JUvg3A%3D%3D\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.134.128, 173.194.210.128, 173.194.214.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.134.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 498862712 (476M) [application/keras]\n",
            "Saving to: ‘model-best.h5.1’\n",
            "\n",
            "model-best.h5.1     100%[===================>] 475.75M   182MB/s    in 2.6s    \n",
            "\n",
            "2021-03-13 17:23:23 (182 MB/s) - ‘model-best.h5.1’ saved [498862712/498862712]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkxkXXHXjaWD"
      },
      "source": [
        "#@title transformer input preparation { form-width: \"25%\" }\r\n",
        "\r\n",
        "import nltk.data\r\n",
        "from  transformers import AutoTokenizer\r\n",
        "\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_str)\r\n",
        "\r\n",
        "def preprocess_bert(text):\r\n",
        "    tokenized_text = tokenizer(text, return_offsets_mapping=True)\r\n",
        "    rows_out = tokenized_text.input_ids\r\n",
        "    return rows_out\r\n",
        "\r\n",
        "def custom_inference(context, question):\r\n",
        "    tokenizer_nltk = nltk.data.load('tokenizers/punkt/english.pickle')  # sentence tokenizer\r\n",
        "    context_sentences = tokenizer_nltk.tokenize(context)\r\n",
        "    preprocessed_context = [\" \".join(str(line).split()) for line in context_sentences]\r\n",
        "    preprocessed_question = \" \".join(str(question).split())\r\n",
        "    tokenized_question = preprocess_bert(preprocessed_question)\r\n",
        "    tokenized_sentences = [preprocess_bert(preprocessed_line) for preprocessed_line in preprocessed_context]\r\n",
        "    sentence_index = 0\r\n",
        "    tokenized_passages = []\r\n",
        "\r\n",
        "    while sentence_index < len(tokenized_sentences):\r\n",
        "        start = sentence_index\r\n",
        "        len_count = len(tokenized_question)\r\n",
        "        while len_count <= 512 and sentence_index < len(tokenized_sentences):\r\n",
        "            len_count += len(tokenized_sentences[sentence_index])\r\n",
        "            sentence_index += 1\r\n",
        "        end = sentence_index -1\r\n",
        "        tokenized_passages.append(preprocess_bert(\" \".join(preprocessed_context[start:end])))\r\n",
        "\r\n",
        "    prob = []\r\n",
        "    candidate_ans = []\r\n",
        "    for tokenized_passage in tokenized_passages:\r\n",
        "        input_ids = tokenized_passage + tokenized_question[1:]\r\n",
        "        token_type_ids = [0] * len(tokenized_passage) + [1] * len(tokenized_question[1:])\r\n",
        "        attention_mask = [1] * len(input_ids)\r\n",
        "        padding_length = max_seq_length - len(input_ids)\r\n",
        "        if padding_length > 0:\r\n",
        "            input_ids = input_ids + ([0] * padding_length)\r\n",
        "            attention_mask = attention_mask + ([0] * padding_length)\r\n",
        "            token_type_ids = token_type_ids + ([0] * padding_length)\r\n",
        "        input_word_ids = np.array(input_ids)\r\n",
        "        input_mask = np.array(attention_mask)\r\n",
        "        input_type_ids = np.array(token_type_ids)\r\n",
        "        predictions = model.predict([np.expand_dims(input_word_ids, axis =0), \r\n",
        "                                    np.expand_dims(input_mask, axis = 0), \r\n",
        "                                    np.expand_dims(input_type_ids,axis=0)])\r\n",
        "        start, end = list(np.argmax(predictions, axis=-1).squeeze())\r\n",
        "        if start > end:\r\n",
        "            continue \r\n",
        "        else:\r\n",
        "            prob_start,prob_end = list(np.max(predictions, axis=-1).squeeze())\r\n",
        "            prob_sum = prob_start+prob_end\r\n",
        "            predicted_ans = tokenizer.decode(tokenized_passage[start : end+1])\r\n",
        "            if predicted_ans != '' and predicted_ans != \"<s>\":\r\n",
        "                candidate_ans.append(predicted_ans)\r\n",
        "                prob.append(prob_sum)\r\n",
        "\r\n",
        "    print(*zip(prob, candidate_ans), sep='\\n')  \r\n",
        "    try:     \r\n",
        "        ans = candidate_ans[np.argmax(prob)]\r\n",
        "    except:\r\n",
        "        ans = \"I'm really sorry, I wasn't able to find an answer :(\"\r\n",
        "    return ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEiRpJpMjbrA",
        "outputId": "7fbfde18-91d5-4282-d339-5c094f07f065"
      },
      "source": [
        "#@title custom inference { form-width: \"25%\" }\r\n",
        "\r\n",
        "questions = [\"Who is the first bride of Tom Cruise?\", \"When was Tom Cruise born?\", \"What is Mission Impossible?\", \"What does Tom Cruise believe in?\", \"What is the genre of Mission Impossible?\", \"Who is the villain in Mission impossible 3?\", \"Who is the villain in Mission impossible ghost protocol?\" \"What is Tom Cruise character's name in Mission Impossible III?\", \"What is Vanilla Sky?\", \"Who directed Mission Impossible?\"]\r\n",
        "question = questions[0]\r\n",
        "question_vectorized = vectorizer.transform([question])\r\n",
        "result = cosine_similarity(question_vectorized,passages_vectorized)\r\n",
        "index = np.argmax(result[0])\r\n",
        "print(\"TF-IDF result:\")\r\n",
        "print(question, end = \" ---> \")\r\n",
        "print(titles[index])\r\n",
        "context = pages_text[titles[index]]\r\n",
        "predicted_ans = custom_inference(context,question)\r\n",
        "print()\r\n",
        "print(\"BERT answer:\")\r\n",
        "print(predicted_ans)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF-IDF result:\n",
            "Who is the first bride of Tom Cruise? ---> Tom Cruise\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "(1.5755904, ' Mimi Rogers')\n",
            "(1.9074783, ' Jack South')\n",
            "(1.8383498, ' Nicole Kidman')\n",
            "(1.4113784, ' Julian Sands')\n",
            "(1.4067633, ' Cameron Diaz')\n",
            "(1.7063069, ' Kathryn Bigelow')\n",
            "(1.9589767, ' Mimi Rogers')\n",
            "(1.3722897, ' Katie Holmes')\n",
            "(1.9474514, ' Mimi Rogers')\n",
            "(1.8999519, ' Kidman')\n",
            "\n",
            "BERT answer:\n",
            " Mimi Rogers\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}